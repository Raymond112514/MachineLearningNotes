{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2afb2909",
   "metadata": {},
   "source": [
    "# Variational inference"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efd13db2",
   "metadata": {},
   "source": [
    "In the previous section, we looked at the generalized EM algorithm. One problem with the algorithm is that it can only be applied when the latent variable is discrete. When latent variable is continuous, then the posterior becomes\n",
    "\n",
    "$$p(z|x) = \\frac{p(x|z)p(z)}{\\int_z p(x|z)p(z) dz}$$\n",
    "\n",
    "In the previous section, we derived the lower bound\n",
    "\n",
    "$$\\log p(x) \\geq \\sum_j \\log\\bigg(\\frac{p_\\theta(x, z=j)}{q(z=j)}\\bigg)q(z=j)$$\n",
    "\n",
    "Whose continuous counterpart is \n",
    "\n",
    "$$\\log p(x) \\geq \\int_z \\log\\bigg(\\frac{p_\\theta(x, z)}{q(z)}\\bigg)q(z)$$\n",
    "\n",
    "In the discrete case, recall that we choose $q(z)=p(z|x)$ so that the bound is tight. Now this is no longer possible since computing the posterior becomes intractable as it involves estimating the integral. Therefore, we can only find a $q$ that makes the lower bound as tight as possible. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
