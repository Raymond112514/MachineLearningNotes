{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "60290a67",
   "metadata": {},
   "source": [
    "# Bias variance tradeoff"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b76910d",
   "metadata": {},
   "source": [
    "Recall that our predictor is determined by solving the empirical minimization problem:\n",
    "\n",
    "$$ \\hat{f} = \\underset{f}{\\text{argmin}}\\; \\frac{1}{n}\\sum_{i=1}^n \\ell(f(X_i), y_i)$$\n",
    "\n",
    "However, what we ultimately aim for is to minimize the expected risk:\n",
    "\n",
    "$$R(f) = \\mathbb{E}[\\ell(f(X), Y)]$$\n",
    "\n",
    "It's possible that we successfully minimize the empirical risk but still have a large expected risk. In such cases, we aren't truly \"learning.\" Therefore, it's crucial to evaluate the generalization error of our predictor, denoted as $R(\\hat{f})$. For regression problems (e.g., square loss functions), there exists a convenient way to decompose the generalization error. This decomposition helps us understand how the generalization error behaves as the model complexity changes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d2a1b7f",
   "metadata": {},
   "source": [
    "## Bias variance decomposition"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edd5b678",
   "metadata": {},
   "source": [
    "For the rest of this section, we assume that we are working with regression problems. In other words, we assume that the loss function is the square loss function. Recall that in the previous section, we showed that the Bayes optimal predictor for regression problem is given by the conditional expectation\n",
    "\n",
    "$$f^*(x)=\\mathbb{E}[Y|X=x]$$\n",
    "\n",
    "In this section, we make another assumption that\n",
    "\n",
    "$$Y=f^*(X)+\\epsilon$$\n",
    "\n",
    "Where $\\epsilon$ is some random variable with $\\mathbb{E}[\\epsilon]=0, \\text{Var}(\\epsilon)=\\sigma^2$. Let $\\hat{f}$ denote our solved predictor. The generalization error is given by \n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "R(\\hat{f}) &= \\mathbb{E}[(\\hat{f}(X)-Y)^2]\\\\\n",
    "&= \\mathbb{E}[(\\hat{f}(X)-f^*(X)-\\epsilon)^2]\\\\\n",
    "&= \\mathbb{E}[(\\hat{f}(X)-f^*(X))^2]-2\\mathbb{E}[\\epsilon(\\hat{f}(X)-f^*(X))] + \\mathbb{E}[\\epsilon^2]\\\\\n",
    "&= \\mathbb{E}[(\\hat{f}(X)-f^*(X))^2] - 2\\mathbb{E}[\\epsilon]\\mathbb{E}[\\hat{f}(X)-f^*(X)] + \\mathbb{E}[\\epsilon^2]\\\\\n",
    "&= \\mathbb{E}[(\\hat{f}(X)-f^*(X))^2] + \\sigma^2\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "Next, since $\\hat{f}$ depends on the observations, it is a random quantity. Therefore, by adding and subtracting its expectation, we have\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "R(\\hat{f}) &= \\mathbb{E}[(\\hat{f}(X)-f^*(X))^2] + \\sigma^2\\\\\n",
    "&= \\mathbb{E}[(\\hat{f}(X)-\\mathbb{E}[(\\hat{f}(X)]+\\mathbb{E}[(\\hat{f}(X)]-f^*(X))^2] + \\sigma^2\\\\\n",
    "&= \\mathbb{E}[(\\hat{f}(X)-\\mathbb{E}[(\\hat{f}(X)])^2] + \\mathbb{E}[(\\mathbb{E}[(\\hat{f}(X)]-f^*(X))^2] + \\sigma^2\\\\\n",
    "&= \\text{Var}(\\hat{f}(X)) + (\\text{Bias}(\\hat{f}(X)))^2 + \\sigma^2\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "Where in the above argument, we ignored the cross product term in line 3 as it evaluates to $0$. This suggests that\n",
    "\n",
    "$$\\text{Generalization error} = \\text{Variance} + \\text{Bias}^2 + \\text{Irreducible noise}$$\n",
    "\n",
    "Since the last term is irreducible, the minimize the generalization error, we want our predictor to have \n",
    "1. Low bias: Meaning that on average, the prediction $\\hat{f}(X)$ is around the true value $f^*(X)$\n",
    "2. Low variance: Meaning that if train the model on different samples, we should get similar results.\n",
    "\n",
    "However, it turns out that in practice, it is hard to acheive low bias and low variance at the same time. More often, the generalization error curve takes in a U-shape (Figure 1).\n",
    "\n",
    "<img src=\"images/tradeoff.png\" style=\"width: 30%; height: auto;\">\n",
    "\n",
    "We observe that\n",
    "\n",
    "* Low model complexity $\\implies$ high bias and low variance\n",
    "* High model complexity $\\implies$ low bias and high variance\n",
    "\n",
    "Therefore, it is more desirable to balance the levels of bias and variance of our predictor. In practice, this is done by evaluating the model error on a validation dataset. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}