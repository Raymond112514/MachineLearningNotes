{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "da2bc93d",
   "metadata": {},
   "source": [
    "# Empirical Risk Minimization (ERM)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa493ab0",
   "metadata": {},
   "source": [
    "## Risk minimization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d278cdfc",
   "metadata": {},
   "source": [
    "Let $\\mathcal{X}$ $\\mathcal{Y}$ denote the input, response distribution. In machine learning, given $X\\sim \\mathcal{X}, Y\\sim \\mathcal{Y}$, the main goal is to learn to predict the response $Y$ given the input $X$. In other words, we want to find a predictor $f:\\mathcal{X}\\rightarrow \\mathcal{Y}$ such that $f(X)$ predicts $Y$ nicely. To measure how \"nice\" a predictor is using a loss function $\\ell: \\mathcal{Y}\\times \\mathcal{Y}\\rightarrow \\mathbb{R}_{\\geq 0}$. If $\\ell(f(X), Y)$ is small on average (note that $X$, $Y$ are random variables), then $f$ is a good predictor. We can frame this as an optimization problem \n",
    "\n",
    "$$f^* = \\underset{f}{\\text{argmin}}\\; \\underbrace{\\mathbb{E}_{(X, Y)\\sim \\mathcal{X}\\times \\mathcal{Y}}[\\ell(f(X), Y)]}_{R(f)} \\tag{1}$$\n",
    "\n",
    "Where $R(f)$ denote the risk, or the average loss, of a predictor. Note that this optimization problem is extremely hard to solve, for three main reasons\n",
    "\n",
    "1. The distribution $\\mathcal{X}$ and $\\mathcal{Y}$ is typically unknown in practice.\n",
    "2. Even if the distribution $\\mathcal{X}, \\mathcal{Y}$ is known, solving the optimization problem requires minimizing a function inside a double integral, which is a hard problem. \n",
    "3. The space of all feasible fucntion is a large set (typically the set of all measurable functions).\n",
    "\n",
    "This means that in practice, we often cannot solve $(1)$ exactly. We need to simiplfy the optimziation problem."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf378c18",
   "metadata": {},
   "source": [
    "## Empirical risk minimization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "520442c7",
   "metadata": {},
   "source": [
    "Even though the distribution $\\mathcal{X}, \\mathcal{Y}$ is unknown in practice, we do have access to the samples $\\mathcal{D}=\\{(X_i, y_i)\\}_{i\\in [n]}\\sim \\mathcal{X}\\times \\mathcal{Y}$. Since under certain regularity conditions\n",
    "\n",
    "$$\\frac{1}{n}\\sum_{i=1}^n \\ell(f(X_i), y_i)\\;\\xrightarrow{\\;\\;n \\to \\infty\\;\\;}\\;\\mathbb{E}_{(X, Y)\\sim \\mathcal{X}\\times \\mathcal{Y}}[\\ell(f(X), Y)]$$\n",
    "\n",
    "This motivates us to replace the objective in $(1)$ as \n",
    "\n",
    "$$\\hat{f} =  \\underset{f}{\\text{argmin}}\\; \\underbrace{\\frac{1}{n}\\sum_{i=1}^n \\ell(f(X_i), y_i)}_{\\hat{R}(f)} \\tag{2}$$\n",
    "\n",
    "Where $\\hat{R}(f)$ is called the empirical risk, it is an estimate of the true risk. This new optimization problem solves problem 1 and problem 2. The size of feasible function is still too large."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b14826c3",
   "metadata": {},
   "source": [
    "## Constrain feasible function set"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fce31342",
   "metadata": {},
   "source": [
    "Since the size of feasible function set is still too large, a natural resolution is to consider a constrained feasible set $\\mathcal{F}$\n",
    "\n",
    "$$\\hat{f}_{\\mathcal{F}} = \\underset{f\\in \\mathcal{F}}{\\text{argmin}}\\; \\frac{1}{n}\\sum_{i=1}^n \\ell(f(X_i), y_i) \\tag{3}$$\n",
    "\n",
    "If the set $\\mathcal{F}$ is not parameterized, then solving $(3)$ gives us a nonparameteric model. On the other hand, if $\\mathcal{F}$ is parameterized by $\\theta\\in \\Theta$, then the final estimate is called a parametric model. In this case, finding the optimal function is equivalent to finding the optimial parameter\n",
    "\n",
    "$$\\hat{\\theta} = \\underset{\\theta \\in \\Theta}{\\text{argmin}}\\; \\frac{1}{n}\\sum_{i=1}^n \\ell(f_{\\theta}(X_i), y_i) \\tag{4}$$\n",
    "\n",
    "Note that problem $(4)$ can be solved numerically using algorithms such as gradient descent and stochastic gradient descent. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d33faeb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6d5ee82",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}