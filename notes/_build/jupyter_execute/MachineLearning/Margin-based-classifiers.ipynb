{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0f3a76bb",
   "metadata": {},
   "source": [
    "# Margin-based classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6b0d5b1",
   "metadata": {},
   "source": [
    "The classifiers we've discussed so far are probabilistic, involving the modeling of the posterior distribution \\(\\mathbb{P}\\{Y=k|X\\}\\) and using it during inference. In contrast, margin-based classifiers are non-probabilistic methods that leverage the geometric properties of the dataset. In general, these classifiers attempts to find a hyperplane that separates the data points. Before diving into the mathematcis, we first review some Euclidean geometry. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a30d3808",
   "metadata": {},
   "source": [
    "## A review of Euclidean geometry"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bcbff8e",
   "metadata": {},
   "source": [
    "In Euclidean space $\\mathbb{R}^n$, a hyperplane is determined by the normal vector $w$ and bias vector $b$. The set of points lying on the hyperplane $\\mathcal{H}$ is given by \n",
    "\n",
    "$$\\mathcal{H}=\\{x:w^Tx+b=0\\}$$\n",
    "\n",
    "The hyperplane $\\mathcal{H}$ separates the original space $\\mathbb{R}^n$ into two parts: \n",
    "\n",
    "$$\\mathcal{H}_+=\\{x:w^Tx+b>0\\}\\hspace{10mm}\\mathcal{H}_-=\\{x:w^Tx+b<0\\}$$\n",
    "\n",
    "If $z$ is a point not on the hyperplane, we can compute its shortest distance $\\gamma$ to the hyperplane. To find $\\gamma$, note that the point $z-\\gamma \\frac{w}{||w||}$ must lie on the hyperplane, this suggests that\n",
    "\n",
    "$$w^T(z-\\gamma\\frac{w}{||w||})+b=0\\implies \\gamma = \\frac{|w^Tz + b|}{||w||}$$\n",
    "\n",
    "Note that an absolute value is included as $z$ can lie on either side of the hyperplane. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78b3db6d",
   "metadata": {},
   "source": [
    "## Seperating data points"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c89bcbce",
   "metadata": {},
   "source": [
    "Recall that our goal is to find a hyperplane that separates the data points. To do so, we need to find a way of describing whether a hyperplane separates the data or not. For the rest of this section, we assume that there exists a hyperplane that separates our data points $\\{(X_i, y_i)\\}_{i\\in [n]}$. Moreover, let $y_i=\\{1, -1\\}$ be the class labels. <br>\n",
    "\n",
    "Note that a hyperplane $\\mathcal{H}$ separates the dataset if all points with label $1$ lies on one side of the half-plane and the points with label $-1$ lies on the other side. Without loss of generality, assume that points with label $1$ lies in $\\mathcal{H}_+$ and points with label $-1$ lies in $\\mathcal{H}_-$. This means that\n",
    "\n",
    "$$\n",
    "\\begin{equation*}\n",
    "\\begin{cases}\n",
    "w^Tx_i+b>0\\hspace{5mm}\\text{if}\\;y_i=1\\\\\n",
    "w^Tx_i+b<0\\hspace{5mm}\\text{if}\\;y_i=-1\n",
    "\\end{cases}\n",
    "\\end{equation*}\n",
    "$$\n",
    "\n",
    "This can be written compactly as \n",
    "\n",
    "$$y_i(w^Tx_i+b)>0\\hspace{5mm}\\text{for all}\\;i\\in [n]\\tag{*}$$\n",
    "\n",
    "Therefore, for a general margin-based classifer, the problem is to find weight $w$ and bias $b$ such that $(*)$ is satisfied. Since there isn't an obvious loss function, we need to use optimization algorithms other than gradient descent. Perceptron algorithm discussed in the next section provides a way of finding $w, b$ in finitely many iterations. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}