{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f0a89149",
   "metadata": {},
   "source": [
    "# Generalized EM algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb4f8013",
   "metadata": {},
   "source": [
    "For the setup, assume we have the parameterized distributions $p(z), p_{\\theta}(x|z),$ where $z$ denote some latent variable. Our goal is to make inference on the unobserved latent variable given some observed data $\\{x_i\\}_{i\\in [n]}$. Formally, we want to estimate\n",
    "\n",
    "$$p(z=j|x) = \\frac{p_{\\theta}(x|z=j)p(z=j)}{\\sum_j p_{\\theta}(x|z=j)p(z=j)}$$\n",
    "\n",
    "Here we are assuming that $z$ takes on finitely many values, the case for infinitely many values is discussed in the next section. Therefore, if we know the distribution $p(z), p_{\\theta}(x|z),$ then computing the posterior is trivial. The real problem, is, therefore, how to estimate the parameters $\\theta$, giving only partially observed data. <br>\n",
    "\n",
    "To do so, a natural starting point is considering the log-likelihood\n",
    "\n",
    "$$\\log p(x) = \\log \\sum_j p_\\theta(x, z=j) = \\log \\sum_j p_\\theta (x|z=j)p(z=j)$$\n",
    "\n",
    "One way of optimizing over $\\theta$ is use a combination of maximum likelihood estimation and gradient descent. We discuss the EM approach like in the previous section. The difficulty of optimizing the expression is that the sum is within the logarithm, and therefore is hard to pull out. We can migitate this by consider likelihood weighing. Given any distribution $q(z=j)$, we have\n",
    "\n",
    "$$\\begin{align*}\n",
    "   \\log p(x) &= \\log \\sum_j p_\\theta(x, z=j)\\\\\n",
    "   &=  \\log \\sum_j p_\\theta(x, z=j) \\frac{q(z=j)}{q(z=j)}\\\\\n",
    "   &\\geq \\sum_j \\log\\bigg(\\frac{p_\\theta(x, z=j)}{q(z=j)}\\bigg)q(z=j)\n",
    "\\end{align*}$$\n",
    "\n",
    "Where the third line we used Jensen's inequality. The lower bound holds for all $q$. We can choose $q$ to make the lower bound tight. Recall that Jensen's inequality equality holds when $X$ constant almost everywhere. Therefore, we choose $q(z=j)$ so that\n",
    "\n",
    "$$q(z=j)\\propto p_\\theta(x, z=j)\\propto p(z=j|x)$$\n",
    "\n",
    "Choosing $q(z=j)$ to be the posterior $p(z=j|x)$ therefore gives us a tight bound\n",
    "\n",
    "$$\\begin{align*}\n",
    "   \\log p(x)\n",
    "   &= \\sum_j \\log\\bigg(\\frac{p_\\theta(x, z=j)}{p(z=j|x)}\\bigg)p(z=j|x)\\\\\n",
    "   &= \\sum_j \\log p_\\theta(x, z=j)p(z=j|x) - \\sum_j p(z=j|x)p(z=j|x)\n",
    "\\end{align*}$$\n",
    "\n",
    "We can now optimize the log-likelihood iteratively. At the beginning, we randomly initialize the parameters. In the E step, using the old parmaters $\\theta_{old}$\n",
    "\n",
    "$$p_{\\text{old}}(z=j|x) = \\frac{p_{\\theta_{old}}(x|z=j)p(z=j)}{\\sum_j p_{\\theta_{old}}(x|z=j)p(z=j)}$$\n",
    "\n",
    "In the M step, we determine new parameters $\\theta_{new}$ by maximizing the log-likelihood\n",
    "\n",
    "$$\\theta_{\\text{new}} = \\underset{\\theta}{\\arg\\max}\\; \\sum_j \\log p_{\\theta_{old}}(x, z=j)p_{\\text{old}}(z=j|x)$$"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}