{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2afb2909",
   "metadata": {},
   "source": [
    "# Variational inference"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efd13db2",
   "metadata": {},
   "source": [
    "In the previous section, we looked at the generalized EM algorithm. One problem with the algorithm is that it can only be applied when the latent variable is discrete. When latent variable is continuous, then the posterior becomes\n",
    "\n",
    "$$p(z|x) = \\frac{p(x|z)p(z)}{\\int_z p(x|z)p(z) dz}$$\n",
    "\n",
    "In the previous section, we derived the lower bound\n",
    "\n",
    "$$\\log p(x) \\geq \\sum_j \\log\\bigg(\\frac{p_\\theta(x, z=j)}{q(z=j)}\\bigg)q(z=j)$$\n",
    "\n",
    "Whose continuous counterpart is \n",
    "\n",
    "$$\\log p(x) \\geq \\int_z \\log\\bigg(\\frac{p_\\theta(x, z)}{q(z)}\\bigg)q(z) = \\int_z \\log p_\\theta(x, z) q(z) -\\int_z \\log q(z)q(z)$$\n",
    "\n",
    "We call the lower bound the evidence lower bound (ELBO). In the discrete case, recall that we choose $q(z)=p(z|x)$ so that ELBO is tight. Now this is no longer possible since computing the posterior becomes intractable as it involves estimating the integral. Therefore, consider a family of \"managable\" distributions $\\mathcal{Q}$, we want to find $q\\in \\mathcal{Q}$ that maximizes the lower bound. To find the optimal parameter, we then find the optimum over the tightest bound. Formally\n",
    "\n",
    "$$\\max_{\\theta}\\max_{q\\in \\mathcal{Q}} \\int_z \\log p_\\theta(x, z) q(z) -\\int_z \\log q(z)q(z)$$\n",
    "\n",
    "The outer maximization problem is a parameter estimation problem, however, the inner maximization problem is a variational problem (that's why it's called variational inference). To simplify the inner maximization, one can also parameterize the distribution class $\\mathcal{Q}$. <br>\n",
    "\n",
    "We can also view the ELBO differently. Note that we can further simplify the ELBO to \n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\text{ELBO} &= \\int_z \\log p_\\theta(x, z) q(z) -\\int_z \\log q(z)q(z) \\\\\n",
    "&= \\int_z \\log [p_\\theta(z|x)p(x)] q(z) -\\int_z \\log q(z)q(z)\\\\\n",
    "&=  \\int_z \\log p_\\theta(z|x) q(z)dz +\\int_z \\log p(x) q(z)dz-\\int_z \\log q(z)q(z)dz\\\\\n",
    "&=  \\int_z \\log p_\\theta(z|x) q(z)dz -\\int_z \\log q(z)q(z)dz + \\log p(x)\\\\\n",
    "&= -\\text{KL}(q||p) + \\log p(x)\n",
    "\\end{align*}$$\n",
    "\n",
    "Therefore, maximizing the ELBO over $q\\in \\mathcal{Q}$ is equivalent as minimizing the KL-divergence between $q(z)$ and $p(z|x)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d0b290d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
