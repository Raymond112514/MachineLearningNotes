{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "da2bc93d",
   "metadata": {},
   "source": [
    "# Empirical Risk Minimization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa493ab0",
   "metadata": {},
   "source": [
    "## Risk minimization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d278cdfc",
   "metadata": {},
   "source": [
    "Recall that our goal is to find a predictor $f: \\mathcal{X} \\rightarrow \\mathcal{Y}$ such that $f(X)$ accurately predicts $Y$. To measure the level of accuray, we introduce a loss function $\\ell: \\mathcal{Y}\\times \\mathcal{Y}\\rightarrow \\mathbb{R}_{\\geq 0}$. The value $\\ell(f(X), Y)$ is large if $f$ is a poor predictor function. In practice, many loss functions are used depending on the context, for example\n",
    "\n",
    "1. Square loss: $\\ell(y, z)=(y-z)^2$\n",
    "2. Zero-one loss: $\\ell(y, z)= \\mathbb{1}\\{y\\neq z\\}$\n",
    "3. Absolute loss: $\\ell(y, z)=|y-z|$\n",
    "\n",
    "We want to find a predictor such that $\\ell(f(X), Y)$ is small on average. This can be framed as an optimization problem:\n",
    "\n",
    "$$f^* = \\underset{f}{\\text{argmin}}\\; \\underbrace{\\mathbb{E}_{(X, Y)\\sim \\mathcal{X}\\times \\mathcal{Y}}[\\ell(f(X), Y)]}_{R(f)} \\tag{1}$$\n",
    "\n",
    "Note that expectation is included since $X, Y$ are random variables. The term $R(f)$ denote the risk, or the expected loss, of the predictor $f$. The solution to $f^*$ is called the Bayes optimal predictor. For certain loss functions, $f^*$ can be determined directly <br>\n",
    "\n",
    "<b>Example 1 (Square loss function) </b>\n",
    "When $\\ell(y, z)=(y-z)^2$, the Bayes optimal predictor is given by\n",
    "\n",
    "$$ f^*(x)=\\mathbb{E}[Y|X=x]$$\n",
    "\n",
    "<i>Proof.</i> <br>\n",
    "Instead of solving $(1)$ directly, note that we can solve it in a point-wise manner\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "f^*(x) &= \\underset{f}{\\text{argmin}}\\; \\mathbb{E}[(f(X)- Y)^2|X=x]\\\\\n",
    "&= \\underset{z\\in \\mathcal{Y}}{\\text{argmin}}\\; \\mathbb{E}[(z-Y)^2|X=x]\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "The objective can be written as \n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\mathbb{E}[(z-Y)^2|X=x]\n",
    "&= \\mathbb{E}[(z- \\mathbb{E}[Y|X=x] + \\mathbb{E}[Y|X=x] - Y)^2|X=x]\\\\\n",
    "&= \\mathbb{E}[(z- \\mathbb{E}[Y|X=x])^2|X=x] + 2\\mathbb{E}[(z- \\mathbb{E}[Y|X=x])(\\mathbb{E}[Y|X=x]-Y)|X=x] + \\mathbb{E}[(\\mathbb{E}[Y|X=x]-Y)^2|X=x]\\\\\n",
    "&= \\mathbb{E}[(z- \\mathbb{E}[Y|X=x])^2|X=x] + \\mathbb{E}[(\\mathbb{E}[Y|X=x]-Y)^2|X=x]\\\\\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "Since the last term in independent of $z$, it follows that the minimizer is given by $z=\\mathbb{E}[Y|X=x]$. This proves the claim. This example tells us that for regression problems, the Bayes optimal predictor is simply given by the conditional expectation. \n",
    "\n",
    "Despite $(1)$ can be solved for certain loss functions, in general, solving $(1)$ is extremly challenging, for three main reasons\n",
    "\n",
    "1. The feature/response distributions are typically unknown.\n",
    "2. Even when the distributions are known, solving $(1)$ involves minimizing a function within an integral. And unfortunately, there aren't efficient numerical algorithms available for solving these types of problems.\n",
    "3. The space of all feasible functions is a large set.\n",
    "\n",
    "This means that in practice, we often cannot solve $(1)$ exactly. Therefore, we need to find ways to simplify the optimization problem."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf378c18",
   "metadata": {},
   "source": [
    "## Improvement 1: Empirical risk minimization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "520442c7",
   "metadata": {},
   "source": [
    "One crucial observation is that while the distributions are often unknown in practice, we do have access to observations $\\mathcal{D}=\\{(X_i, y_i)\\}_{i\\in [n]}$ sampled from the original distribution. This is important because, under certain regularity conditions, the law of large numbers suggests that\n",
    "\n",
    "$$\\frac{1}{n}\\sum_{i=1}^n \\ell(f(X_i), y_i)\\;\\xrightarrow{\\;\\;n \\to \\infty\\;\\;}\\;\\mathbb{E}_{(X, Y)\\sim \\mathcal{X}\\times \\mathcal{Y}}[\\ell(f(X), Y)]$$\n",
    "\n",
    "This motivates us to replace the objective in $(1)$ as \n",
    "\n",
    "$$\\hat{f} =  \\underset{f}{\\text{argmin}}\\; \\underbrace{\\frac{1}{n}\\sum_{i=1}^n \\ell(f(X_i), y_i)}_{\\hat{R}(f)} \\tag{2}$$\n",
    "\n",
    "Where the term $\\hat{R}(f)$ is called the empirical risk, it is an estimate of the true risk. The optimization problem $(2)$ resolves point 1 and 2 presented earlier, however, the set of feasible function is still a large set. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b14826c3",
   "metadata": {},
   "source": [
    "## Improvement 2: Constrained optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fce31342",
   "metadata": {},
   "source": [
    "A straightforward method to address point 3 is by constraining the set of possible functions. Rather than optimizing over all measurable functions, we confine the feasible function to the set $\\mathcal{F}$. This gives us the following\n",
    "\n",
    "$$\\hat{f}_{\\mathcal{F}} = \\underset{f\\in \\mathcal{F}}{\\text{argmin}}\\; \\frac{1}{n}\\sum_{i=1}^n \\ell(f(X_i), y_i) \\tag{3}$$\n",
    "\n",
    "Optimization problem $(3)$ is in general solvable. If the functions in $\\mathcal{F}$ is parameterized by $\\theta\\in \\Theta$, then $(3)$ is equivalent to a parameter estimation problem\n",
    "\n",
    "$$\\hat{\\theta} = \\underset{\\theta \\in \\Theta}{\\text{argmin}}\\; \\frac{1}{n}\\sum_{i=1}^n \\ell(f_{\\theta}(X_i), y_i)$$\n",
    "\n",
    "Which can be solved using algorithms like stochastic gradient descent. This gives us a parametric model. If, on the other hand, $\\mathcal{F}$ is not parameterized, then by choosing a good set of functions, in general one can solve $(3)$ using other methods. This gives us a nonparametric model. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "742b917d",
   "metadata": {},
   "source": [
    "## Example: Linear regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "839c29d0",
   "metadata": {},
   "source": [
    "Suppose we restrict ourselves on the set of linear functions $\\mathcal{F}=\\{f:f(x)=\\theta^Tx\\}$. Given observations $\\mathcal{D}=\\{(X_i, y_i)\\}_{i\\in [n]}$, and using the square loss, the optimal parameter is given by \n",
    "\n",
    "$$\\hat{\\theta} = \\underset{\\theta}{\\text{argmin}}\\; \\frac{1}{n}\\sum_{i=1}^n (\\theta^TX_i- y_i)^2$$\n",
    "\n",
    "Letting $X, y$ denote the matrix of features/response, the above is equivalent to \n",
    "\n",
    "$$\\hat{\\theta} = \\underset{\\theta}{\\text{argmin}}\\; \\frac{1}{n}||X\\theta-y||_2^2$$\n",
    "\n",
    "This optimization problem can be solved using standard first order methods. From the first order condition\n",
    "\n",
    "$$\\nabla_{\\theta} = 2(X^TX)\\theta - 2(X^Ty) = 0\\implies \\hat{\\theta} = (X^TX)^{-1}(X^Ty)$$\n",
    "\n",
    "And this gives us the optimal predictor under empirical risk minimization. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
