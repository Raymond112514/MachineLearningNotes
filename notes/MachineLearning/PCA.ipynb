{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f23be30f",
   "metadata": {},
   "source": [
    "# Principal component analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4d57b4d",
   "metadata": {},
   "source": [
    "https://www.stat.cmu.edu/~cshalizi/uADA/12/lectures/ch18.pdf\n",
    "\n",
    "In this section, assume the data $\\mathcal{D}=\\{X_i\\}_{i\\in [n]}$ is centered, namely,\n",
    "\n",
    "$$\\mu=\\frac{1}{n}\\sum_i X_i=0$$\n",
    "\n",
    "In principal component analysis, our goal is to find a low dimensional representation of the data. In particular, we want to find a lower dimensional hyperplane that best describes our data. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4f35e97",
   "metadata": {},
   "source": [
    "## Projection to a line"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dca2179",
   "metadata": {},
   "source": [
    "We start with a one dimenional problem: consider projecting the data onto a line. From the figure, it can be seen that choice 3 is a better projection than the rest since it preserves most variance in the data. <br>\n",
    "\n",
    "### View 1: Maximizing variance\n",
    "\n",
    "The projection of $x_i$ onto the line spanned by $w$ is given by $(x_i\\cdot w)$. The mean of these projection is zero since\n",
    "\n",
    "$$\\frac{1}{n}\\sum_{i}(x_i\\cdot w) w = \\Bigg(\\bigg(\\frac{1}{n}\\sum_i x_i\\bigg)\\cdot w\\Bigg)w=0$$\n",
    "\n",
    "Given $w$, the projected points therefore are $z_i=x_i\\cdot w$. We want to find $w$ that maximizes the variance of these scalars. Recall that the variance of a list of number is given by\n",
    "\n",
    "$$\\frac{1}{n}\\sum_{i=1}^n z_i^2 - \\bigg(\\frac{1}{n}\\sum_{i=1}^n z_i\\bigg)^2$$\n",
    "\n",
    "But as we've shown earlier, the mean of the projection is zero. The variance is therefore given by the first term. We can frame this as an optimization problem\n",
    "\n",
    "$$\\max_w \\frac{1}{n}\\sum_{i=1}^n z_i^2 = \\max_w \\frac{1}{n}\\sum_{i=1}^n (x_i\\cdot w)^2$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bedae35",
   "metadata": {},
   "source": [
    "### View 2: Minimizing error\n",
    "\n",
    "We can also develop the same optimization problem by minimizing the error. Note that the error induced by dimensionality reduction is given by $\\epsilon_i = x_i - (x_i\\cdot w) w$. We want to make the overall error small. This gives us the following problem\n",
    "\n",
    "$$\\min_w ||x_i - (x_i\\cdot w) w||_2^2$$\n",
    "\n",
    "Expanding the norm term gives us\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "||x_i - (x_i\\cdot w) w||_2^2 &= x_i^Tx_i - (x_i^Tw)^2 - (x_i^Tw)^2 + (x_i^Tw)^2 w^Tw\\\\\n",
    " &= x_i^Tx_i - (x_i^Tw)^2 \n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "The optimization problem can therefore be rewritten as\n",
    "\n",
    "$$\\min_w x_i^Tx_i - (x_i^Tw)^2 $$\n",
    "\n",
    "Since the first term is independnet of $w$\n",
    "\n",
    "$$\\max_w \\frac{1}{n}\\sum_i(x_i^Tw)^2 $$\n",
    "\n",
    "Which is the same as maximizing the variance. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47b8c908",
   "metadata": {},
   "source": [
    "## Solution methods"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b2cca37",
   "metadata": {},
   "source": [
    "Let $X$ be the matrix created by concatenating the data $x_i$, then the optimization problem can be rewritten as\n",
    "\n",
    "$$\\max_w\\frac{1}{n} w^TX^TXw\\hspace{5mm}\\text{subject to}\\;||w||_2^2=1$$\n",
    "\n",
    "Letting $\\Sigma = (X^TX)/n$ denote the sample covariance matrix, then the Lagrangian is given by\n",
    "\n",
    "$$\\mathcal{L}(w, \\lambda)= w^T\\Sigma w - \\lambda (||w||_2^2-1)$$\n",
    "\n",
    "The first order condition is given by\n",
    "\n",
    "$$\n",
    "\\begin{equation*}\n",
    "\\begin{cases}\n",
    "    \\frac{\\partial \\mathcal{L}}{\\partial w} = 2\\Sigma w - 2\\lambda w = 0\\\\\n",
    "    \\frac{\\partial \\mathcal{L}}{\\partial \\lambda} =||w||_2^2-1=0\n",
    "\\end{cases}\n",
    "\\end{equation*}\n",
    "$$\n",
    "\n",
    "This suggests that $w$ with unit norm is given by \n",
    "\n",
    "$$\\Sigma w = \\lambda w$$\n",
    "\n",
    "Note that $w$ essentially corresponds to the eigenvector of $\\Sigma$, with $\\lambda$ being the corresponding eigenvalue. In this case, the objective becomes\n",
    "\n",
    "$$w^T\\Sigma w=w^T(\\lambda w) = \\lambda $$\n",
    "\n",
    "This implies that we want to choose $w$ t be the eigenvector of $\\Sigma$ with the largest eigenvalue. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "213a1305",
   "metadata": {},
   "source": [
    "## Projection to the hyperplane"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "029ac5dc",
   "metadata": {},
   "source": [
    "We now discuss projection to the hyperplane. Let $w_1, w_2,..., w_K$ be an orthonormal basis of the hyperplane. Then given $x_i$, the projection onto the hyperplane is given by \n",
    "\n",
    "$$\\sum_{k=1}^K (w_k^Tx_i)w_k$$\n",
    "\n",
    "Like before, we want to find $w_1,..., w_K$ that minimizes the reconstruction error\n",
    "\n",
    "$$\\min_{w_1, w_2,.., w_K} \\frac{1}{n}\\sum_{i=1}^n ||x_i-\\sum_{k=1}^K (w_k^Tx_i)w_k||_2^2$$\n",
    "\n",
    "By the same argument as the case of a line, it can be shown that the above is equivalent to \n",
    "\n",
    "$$\\max_{w_1, w_2,.., w_K} \\frac{1}{n}\\sum_{i=1}^n \\sum_{k=1}^K (w_k^Tx_i)^2$$\n",
    "\n",
    "Let $X$ be the design matrix, then we can rewrite the above as\n",
    "\n",
    "$$\\max_{w_1, w_2,.., w_K} \\sum_{k=1}^K w_k^T\\Sigma w_k$$\n",
    "\n",
    "Since $\\Sigma$ is a positive semidefinite matrix, we can diagonalize it $\\Sigma = UDU^T$, where $U$ is orthonormal matrix. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dad91b5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
